% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SparkSimple.R
\name{clusterApply}
\alias{clusterApply}
\title{Parallelize computations using a Spark cluster}
\usage{
clusterApply(cl, x, fun, ...)
}
\arguments{
\item{cl}{cluster is a Spark connection as returned from
\code{\link[sparkapi]{start_shell}}}

\item{x}{R object that can be coerced to list}

\item{fun}{function to evaluate}
}
\value{
list with \code{fun} evaluated at each element of x
}
\description{
This works by serializing x onto the worker nodes, running the
computation, and finally deserializing the result.
}
\examples{
library(sparkapi)
sc <- start_shell(master = "local")

clusterApply(sc, 1:10, function(x) x + 2)

a <- 20
helperfunc <- function(x) sin(x)
f <- function(x) helperfunc(x) + a
# To apply f you'll need to also get it's dependencies
f <- makeClosure(f, c("a", "helperfunc"))
clusterApply(sc, 1:10, f)

}
\seealso{
\code{makeClosure}, \code{\link[base]{lapply}}, 
     \code{\link[parallel]{clusterApply}},
     \code{\link[parallel]{clusterExport}}, in \code{parallel} package
}

